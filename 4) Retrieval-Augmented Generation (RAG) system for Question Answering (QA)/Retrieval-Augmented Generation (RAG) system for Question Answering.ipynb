{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "pu12g_-xnHYo"
      },
      "outputs": [],
      "source": [
        "!pip install langchain==0.1.13 langchain-community langchain-google-genai sentence-transformers chromadb pypdf reportlab"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eg6HWmEsnZk9"
      },
      "source": [
        "Upload PDFs from Local"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hmJ9ZTgbneBk"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "import shutil\n",
        "import os\n",
        "\n",
        "# Remove existing folder if exists\n",
        "if os.path.exists(\"/content/papers\"):\n",
        "    shutil.rmtree(\"/content/papers\")\n",
        "\n",
        "# Create target directory\n",
        "os.makedirs(\"/content/papers\", exist_ok=True)\n",
        "\n",
        "# Upload files\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Move only valid non-empty PDFs into the folder\n",
        "for filename in uploaded.keys():\n",
        "    if filename.lower().endswith('.pdf') and os.path.getsize(filename) > 0:\n",
        "        shutil.move(filename, f\"/content/papers/{filename}\")\n",
        "    else:\n",
        "        print(f\"Skipped invalid or empty file: {filename}\")\n",
        "\n",
        "print(\"Files uploaded successfully.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "weg86P34nz2e"
      },
      "source": [
        "Load and Process PDF Documents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lyzKeQrvn4Dj"
      },
      "outputs": [],
      "source": [
        "from langchain_community.document_loaders import PyPDFDirectoryLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "import glob\n",
        "from pypdf.errors import PdfReadError\n",
        "\n",
        "# Helper: remove truly empty PDFs before loading\n",
        "for pdf_file in glob.glob(\"/content/papers/*.pdf\"):\n",
        "    if os.path.getsize(pdf_file) == 0:\n",
        "        print(f\"Removing empty file: {pdf_file}\")\n",
        "        os.remove(pdf_file)\n",
        "\n",
        "# Load PDFs\n",
        "loader = PyPDFDirectoryLoader(\"/content/papers\")\n",
        "try:\n",
        "    docs = loader.load()\n",
        "except PdfReadError as e:\n",
        "    print(f\"Error reading a PDF: {e}\")\n",
        "\n",
        "# Add metadata\n",
        "for i, doc in enumerate(docs):\n",
        "    source = doc.metadata.get('source', f'doc_{i}.pdf')\n",
        "    doc.metadata['filename'] = source.split('/')[-1]\n",
        "    doc.metadata['page'] = doc.metadata.get('page', i + 1)\n",
        "\n",
        "# Split into text chunks\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=300, chunk_overlap=50)\n",
        "chunks = text_splitter.split_documents(docs)\n",
        "\n",
        "print(f\"Loaded {len(docs)} pages and split into {len(chunks)} chunks.\")\n",
        "print(\"Final list of files to process:\")\n",
        "print(os.listdir(\"/content/papers\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gEkwl3Pon8Wz"
      },
      "source": [
        "Generate Embeddings and Build Vectorstore"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fIEQAWOgn9y0"
      },
      "outputs": [],
      "source": [
        "from langchain_community.embeddings import SentenceTransformerEmbeddings\n",
        "from langchain.vectorstores import Chroma\n",
        "\n",
        "# Load embedding model\n",
        "embeddings = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
        "\n",
        "# Create vectorstore from chunks\n",
        "vectorstore = Chroma.from_documents(chunks, embeddings)\n",
        "\n",
        "# Create retriever\n",
        "retriever = vectorstore.as_retriever(search_kwargs={'k': 5})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UE1TEzFHoD5E"
      },
      "source": [
        "  Set Up Gemini API & LLM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5X8Bl_xyoIW0"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# Set your Google API key\n",
        "os.environ[\"GOOGLE_API_KEY\"] = \"AIzaSyDnxnyeCE5v3TsVZBJrol4Q7XJ5KB0VzZ4\"\n",
        "\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "# Initialize LLM\n",
        "llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\", temperature=0.5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nMb_5NGloMxC"
      },
      "source": [
        "Define Prompt and Context Formatter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nX0Af1tIoNBX"
      },
      "outputs": [],
      "source": [
        "from langchain.prompts import ChatPromptTemplate\n",
        "\n",
        "# Prepare context from retrieved docs\n",
        "def prepare_context_with_sources(documents):\n",
        "    context_blocks = []\n",
        "    source_citations = set()\n",
        "\n",
        "    for doc in documents:\n",
        "        filename = doc.metadata.get(\"filename\", \"unknown_file\")\n",
        "        page = doc.metadata.get(\"page\", \"N/A\")\n",
        "        content = doc.page_content.strip().replace(\"\\n\", \" \")\n",
        "\n",
        "        context_blocks.append(f\"[{filename}, Page {page}]: {content}\")\n",
        "        source_citations.add((filename, page))\n",
        "\n",
        "    return \"\\n\\n\".join(context_blocks), source_citations\n",
        "\n",
        "# Define prompt template\n",
        "template = \"\"\"\n",
        "<context>\n",
        "{context}\n",
        "</context>\n",
        "\n",
        "You are an AI assistant answering questions based on academic papers.\n",
        "Answer the following question truthfully and clearly using only the above context.\n",
        "Do not hallucinate or make up information.\n",
        "\n",
        "Question: {query}\n",
        "\"\"\"\n",
        "\n",
        "prompt = ChatPromptTemplate.from_template(template)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NYSI8ktwoYly"
      },
      "source": [
        "Define RAG Function & History Tracker"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uifp2QVPoZgR"
      },
      "outputs": [],
      "source": [
        "# Initialize history list\n",
        "qa_history = []\n",
        "\n",
        "# Function to run RAG + source citations\n",
        "def rag_with_sources(query):\n",
        "    docs = retriever.get_relevant_documents(query)\n",
        "    context, sources = prepare_context_with_sources(docs)\n",
        "\n",
        "    inputs = {\"context\": context, \"query\": query}\n",
        "    answer = llm.invoke(prompt.format_prompt(**inputs).to_messages())\n",
        "\n",
        "    formatted_sources = [f\"{file}, Page {page}\" for file, page in sources]\n",
        "    qa_entry = {\n",
        "        \"question\": query,\n",
        "        \"answer\": answer.content.strip(),\n",
        "        \"sources\": formatted_sources\n",
        "    }\n",
        "    qa_history.append(qa_entry)\n",
        "\n",
        "    return qa_entry"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rb46e3Z9ob2V"
      },
      "source": [
        "Sample questions test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "sRTABs8Zw425"
      },
      "outputs": [],
      "source": [
        "sample_questions = [\n",
        "    \"What are the main components of a RAG model, and how do they interact?\",\n",
        "    \"What are the two sub-layers in each encoder layer of the Transformer model?\",\n",
        "    \"Explain how positional encoding is implemented in Transformers and why it is necessary.\",\n",
        "    \"Describe the concept of multi-head attention in the Transformer architecture. Why is it beneficial?\",\n",
        "    \"What is few-shot learning, and how does GPT-3 implement it during inference?\"\n",
        "]\n",
        "for q in sample_questions:\n",
        "    result = rag_with_sources(q)\n",
        "    print(f\"\\nQ: {result['question']}\\nA: {result['answer']}\\nSources: {', '.join(result['sources'])}\\n\")\n",
        "\n",
        "# Interactive loop\n",
        "import sys\n",
        "while True:\n",
        "    user_input = input(\"Ask a question (or type 'exit'): \")\n",
        "    if user_input.lower() == \"exit\":\n",
        "        print(\"Exiting Q&A.\")\n",
        "        break\n",
        "    if user_input.strip() == \"\":\n",
        "        continue\n",
        "    result = rag_with_sources(user_input)\n",
        "    print(f\"\\nQ: {result['question']}\\nA: {result['answer']}\\nSources: {', '.join(result['sources'])}\\n\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
